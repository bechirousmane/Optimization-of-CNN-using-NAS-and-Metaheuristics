\section{Expériences et résultats}
Afin d'évaluer les performances de deux algorithmes génétiques et firefly, nous avons mener des expériences sur deux jeux de données, un sous échantillons du jeu de données MNIST\cite{lecun1989backpropagation} incluant 5 classes choisis aléatoirement pour chaque expériences, nous avons ensuite mené une seule expérience sur le jeu données CIFAR-10\cite{krizhevsky2009learning}.Ces choix ont été motivés par les contraintes de ressources computationnelles. Nous avons utiliser la recherche aléatoire comme méthode de référence (\textit{baseline}) pour la comparaison. L’objectif est de maximiser une fonction de fitness définie comme suit :
\begin{equation}
    f(\text{err}) = \frac{1}{1 + \text{err}}
\end{equation}
où $\text{err}$ est la moyenne de la perte (loss) calculée sur les ensembles d’entraînement et de test.

Les expériences ont été conduites en utilisant une approche parallèle :
\begin{itemize}
    \item 6 cœurs CPU pour les expériences sur MNIST ;
    \item 4 GPU pour les expériences sur CIFAR-10.
\end{itemize}

\subsection{Espace de recherche et configuration}

L'espace de recherche est paramétré selon les éléments suivants :

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Paramètre} & \textbf{MNIST} & \textbf{CIFAR-10} \\
\hline
Nombre de couches (min–max)     & 2 à 10        & 2 à 10 \\
Filtres convolutionnels         & \{1, 8, 16, 32\}    & \{32, 64, 128, 256\} \\
Tailles de noyau (kernel)       & \{2, 3, 5, 7\}      & \{2, 3, 5, 7\} \\
Strides                         & \{1, 2, 3, 4\}      & \{1, 2, 3, 4\} \\
Padding convolutionnel          & 1             & 1 \\
Padding de pooling              & 1             & 1 \\
Fonctions d’activation          & \{ReLU, ELU, Sigmoid, Tanh\} & \{ReLU, ELU, Sigmoid, Tanh\} \\
Taille des couches fully connected & \{8 à 512\} par pas irrégulier  & \{8 à 512\} par pas irrégulier \\
\hline
\end{tabular}
\caption{Espace de recherche défini pour MNIST et CIFAR-10.}
\label{tab:search_space}
\end{table}

\subsection{Configuration des paramètres d’entraînement}

Les paramètres utilisés pour l'entraînement et la recherche sont les suivants :

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Paramètre} & \textbf{MNIST} & \textbf{CIFAR-10} \\
\hline
Taille du batch                & 64        & 128        \\
Nombre d’itérations de recherche & 30         & 15         \\
Taille de la population        & 20         & 20         \\
Nombre d’époques (recherche)   & 10         & 10         \\
Nombre d’époques (final)       & 30         & 50         \\
Optimiseur utilisé             & AdamW      & AdamW      \\
Forme de l’entrée              & (1, 18, 18) & (3, 32, 32) \\
Nombre de classes              & 5          & 10         \\
Nombre d'exemples train        & 30\,524 en moyenne    & 60\,000    \\
Nombre d'exemples test         & 5\,133 en moyenne    & 10\,000    \\
Taux d’apprentissage           & 0.001      & 0.001      \\
\hline
\end{tabular}
\caption{Paramètres d’entraînement utilisés pour les expériences.}
\label{tab:params_general}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Paramètre (Génétique)}       & \textbf{Valeur} \\
\hline
Taux de mutation                     & 0.1             \\
Taille du tournoi                    & 5               \\
Probabilité de croisement           & 0.8             \\
Pression de sélection               & 1.5             \\
\hline
\end{tabular}
\vspace{3cm}
\begin{tabular}{|l|c|}
\hline
\textbf{Paramètre (Luciole)}         & \textbf{Valeur} \\
\hline
$\alpha$ (facteur de randomisation) & 0.5             \\
$\beta_0$ (attractivité initiale)   & 1.0             \\
$\gamma$ (absorption lumineuse)     & 1.0             \\
$\sigma_0$ (écart-type initial)     & 1.0             \\
Probabilité d’utilisation du mouvement normal & 0.5     \\
\hline
\end{tabular}
\caption{Paramètres spécifiques des algorithmes.}
\label{tab:params_algos}
\end{table}

\subsection{Protocole expérimental}

Chaque expérimentation suit les étapes suivantes :
\begin{enumerate}
    \item Génération aléatoire d’une population d’architectures ;
    \item Évaluation de la population sur un sous-ensemble des données d'entraînement et de test ;
    \item Application des mécanismes spécifique à chaque algorithmes;
    \item Sauvegarde des meilleures architectures à chaque génération ;
    \item Entraînement final de la meilleure architecture sur plusieurs époques ;
    \item Évaluation finale sur l’ensemble de test complet.
\end{enumerate}

\subsection{Résultats}
\subsubsection{MNIST}

Les expériences sur le dataset MNIST ont été conduites avec 10 graines aléatoires différentes (seeds 40 à 50, excluant le seed 43) pour chaque algorithme d'optimisation afin d'assurer la robustesse statistique des résultats. Chaque expérience a été menée sur un sous-échantillon de 5 classes choisies aléatoirement à partir du dataset MNIST original, comme décrit dans la section précédente.

\paragraph{Performances détaillées par algorithme}

Le tableau~\ref{tab:mnist_random} présente les résultats obtenus par la recherche aléatoire.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Seed} & \textbf{Score} & \textbf{Précision test (\%)} & \textbf{Perte test} \\
\hline
40 & 0.9920 & 99.12 & 0.0267 \\
41 & 0.9875 & 99.40 & 0.0244 \\
42 & 0.9888 & 99.14 & 0.0342 \\
44 & 0.9891 & 98.98 & 0.0343 \\
45 & 0.9882 & 99.16 & 0.0315 \\
46 & 0.9895 & 99.43 & 0.0251 \\
47 & 0.9917 & 99.56 & 0.0128 \\
48 & 0.9851 & 99.04 & 0.0366 \\
49 & 0.9898 & 99.44 & 0.0151 \\
50 & 0.9878 & 99.44 & 0.0230 \\
\hline
\end{tabular}
\caption{Résultats de la recherche aléatoire sur MNIST}
\label{tab:mnist_random}
\end{table}

Les résultats de l'algorithme génétique sont présentés dans le tableau~\ref{tab:mnist_genetic}. 
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Seed} & \textbf{Score} & \textbf{Précision test (\%)} & \textbf{Perte test} \\
\hline
40 & 0.9953 & 99.59 & 0.0114 \\
41 & 0.9901 & 99.60 & 0.0161 \\
42 & 0.9947 & 99.64 & 0.0124 \\
44 & 0.9937 & 99.60 & 0.0109 \\
45 & 0.9930 & 99.57 & 0.0145 \\
46 & 0.9914 & 99.33 & 0.0303 \\
47 & 0.9962 & 99.62 & 0.0145 \\
48 & 0.9912 & 99.37 & 0.0161 \\
49 & 0.9946 & 99.54 & 0.0151 \\
50 & 0.9937 & 99.62 & 0.0109 \\
\hline
\end{tabular}
\caption{Résultats de l'algorithme génétique sur MNIST}
\label{tab:mnist_genetic}
\end{table}

Le tableau~\ref{tab:mnist_firefly} présente les performances de l'algorithme Firefly.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Seed} & \textbf{Score} & \textbf{Précision test (\%)} & \textbf{Perte test} \\
\hline
40 & 0.9937 & 99.63 & 0.0109 \\
41 & 0.9896 & 99.52 & 0.0152 \\
42 & 0.9891 & 99.44 & 0.0239 \\
44 & 0.9912 & 99.66 & 0.0144 \\
45 & 0.9909 & 99.47 & 0.0156 \\
46 & 0.9906 & 99.55 & 0.0157 \\
47 & 0.9921 & 99.49 & 0.0151 \\
48 & 0.9874 & 99.13 & 0.0397 \\
49 & 0.9921 & 99.48 & 0.0156 \\
50 & 0.9882 & 99.14 & 0.0355 \\
\hline
\end{tabular}
\caption{Résultats de l'algorithme Firefly sur MNIST}
\label{tab:mnist_firefly}
\end{table}

\paragraph{Analyse comparative des performances}

Le tableau~\ref{tab:mnist_summary} synthétise les performances moyennes obtenues par chaque algorithme d'optimisation, accompagnées des écarts-types pour évaluer la stabilité des résultats.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Méthode} & \textbf{Score moyen} & \textbf{Précision (\%)} & \textbf{Perte moyenne} & \textbf{Écart-type} \\
\hline
Recherche aléatoire & 0.9890 & 99.27 & 0.0264 & 0.0021 \\
Algorithme génétique & \textbf{0.9934} & \textbf{99.55} & \textbf{0.0152} & 0.0019 \\
Algorithme Firefly & 0.9905 & 99.45 & 0.0212 & 0.0019 \\
\hline
\end{tabular}
\caption{Synthèse comparative des performances sur MNIST}
\label{tab:mnist_summary}
\end{table}

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{front/synthese_comparative_score_mnist.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{front/synthese_comparative_precision_mnist.png}
    \end{minipage}
    \caption{Comparaison des performances sur MNIST : (gauche) Score moyenne, (droite) Précision moyenne}
    \label{fig:comparison-mnist}
\end{figure}

L'analyse des résultats révèle une hiérarchie claire des performances. L'algorithme génétique obtient les meilleures performances avec un score de fitness moyen de 0.9934 et une précision de test de 99.55\%. Cette supériorité se manifeste également par la perte de test la plus faible (0.0152), témoignant d'une meilleure capacité de généralisation des architectures découvertes.

L'algorithme Firefly se positionne en position intermédiaire avec un score moyen de 0.9905 et une précision de 99.45\%. Bien que ses performances dépassent significativement celles de la recherche aléatoire, l'écart avec l'algorithme génétique demeure notable sur l'ensemble des métriques évaluées.

La recherche aléatoire, utilisée comme référence, atteint un score moyen de 0.9890 et une précision de 99.27\%. Ces résultats, bien qu'élevés en valeur absolue, confirment l'intérêt des approches métaheuristiques pour l'optimisation d'architectures neurales.

\paragraph{Significance statistique des améliorations}

L'amélioration apportée par l'algorithme génétique par rapport à la recherche aléatoire se quantifie comme suit :
\begin{itemize}
    \item Gain de 0.44 points de pourcentage en score de fitness
    \item Amélioration de 0.28 points de pourcentage en précision de test  
    \item Réduction de 42.3\% de la perte de test
\end{itemize}

La stabilité des résultats est confirmée par les faibles écarts-types observés (inférieurs à 0.0021 pour tous les algorithmes), attestant de la robustesse des conclusions tirées sur l'ensemble des graines aléatoires testées.

Ces résultats démontrent l'efficacité de l'optimisation par algorithme génétique pour la recherche automatique d'architectures de réseaux de neurones convolutionnels. Les gains obtenus, bien que modérés en valeur absolue compte tenu des performances déjà élevées sur MNIST, sont statistiquement significatifs et cohérents avec la complexité relativement faible de ce dataset de référence.

\subsubsection{CIFAR-10}

Les expériences sur le dataset CIFAR-10 ont été menées avec une configuration adaptée à la complexité accrue de ce dataset par rapport à MNIST. Une seule expérience a été réalisée en raison des contraintes de ressources computationnelles, utilisant 4 GPU pour l'entraînement parallèle comme spécifié dans la configuration expérimentale.

\paragraph{Performances de recherche d'architecture}

Le tableau~\ref{tab:cifar_search_results} présente les performances obtenues par chaque algorithme lors de la phase de recherche d'architecture, incluant le score de fitness atteint et le nombre de modèles évalués.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Méthode} & \textbf{Score de fitness} & \textbf{Modèles évalués} & \textbf{Temps d'exécution} \\
\hline
Recherche aléatoire & 0.5904 & 194 & $\sim$3h 22min \\
Algorithme génétique & \textbf{0.6569} & 278 & $\sim$5h 00min \\
Algorithme Firefly & 0.6140 & 327 & $\sim$6h 10min \\
\hline
\end{tabular}
\caption{Performances des algorithmes lors de la recherche d'architecture sur CIFAR-10}
\label{tab:cifar_search_results}
\end{table}

L'algorithme génétique démontre une supériorité claire avec un score de fitness de 0.6569, soit une amélioration de 11.3\% par rapport à la recherche aléatoire et de 7.0\% par rapport à l'algorithme Firefly. Cette performance est obtenue avec un nombre modéré d'évaluations (278), témoignant de l'efficacité de l'exploration guidée par les mécanismes évolutionnaires.

\paragraph{Architectures optimales découvertes}

L'analyse des architectures optimales révèle des stratégies distinctes adoptées par chaque algorithme d'optimisation. Le tableau~\ref{tab:cifar_architectures} présente la structure des meilleures architectures découvertes par chaque méthode.

\begin{table}[H]
\centering
\scriptsize
\begin{tabular}{|p{3cm}|p{10cm}|}
\hline
\textbf{Méthode} & \textbf{Architecture découverte} \\
\hline
Recherche aléatoire & Conv(128 filtres, noyau 5×5, stride 2) $\rightarrow$ Conv(256, 5×5, stride 1) $\rightarrow$ MaxPool(2×2, stride 2) $\rightarrow$ Conv(64, 5×5, stride 1) $\rightarrow$ FC(224, ELU) $\rightarrow$ FC(80, Sigmoid) \\
\hline
Algorithme génétique & Conv(64, 3×3, stride 1) $\rightarrow$ Conv(256, 3×3, stride 2) $\rightarrow$ Conv(128, 3×3, stride 2) $\rightarrow$ Conv(128, 3×3, stride 1) $\rightarrow$ Conv(256, 3×3, stride 1) $\rightarrow$ FC(384, ReLU) \\
\hline
Algorithme Firefly & Conv(32, 2×2, stride 1) $\rightarrow$ Conv(32, 5×5, stride 1) $\rightarrow$ MaxPool(3×3, stride 1) $\rightarrow$ Conv(128, 2×2, stride 2) $\rightarrow$ Conv(128, 3×3, stride 3) $\rightarrow$ Conv(64, 5×5, stride 1) $\rightarrow$ MaxPool(3×3, stride 2) \\
\hline
\end{tabular}
\caption{Architectures optimales découvertes sur CIFAR-10}
\label{tab:cifar_architectures}
\end{table}

\paragraph{Évaluation des performances finales}

Le tableau~\ref{tab:cifar_final_results} présente les performances obtenues après entraînement complet des meilleures architectures découvertes.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Méthode} & \textbf{Précision train (\%)} & \textbf{Perte train} & \textbf{Précision test (\%)} & \textbf{Perte test} \\
\hline
Recherche aléatoire & 72.61 & 0.5222 & 79.95 & 0.5891 \\
Algorithme génétique & \textbf{77.94} & \textbf{0.3624} & \textbf{83.37} & \textbf{0.5280} \\
Algorithme Firefly & 74.73 & 0.4747 & 79.75 & 0.6034 \\
\hline
\end{tabular}
\caption{Performances après entraînement standard (15 époques) sur CIFAR-10}
\label{tab:cifar_final_results}
\end{table}

L'évaluation avec un entraînement étendu sur un plus grand nombre d'époques(50 époques) révèle des performances encore supérieures, comme présenté dans le tableau~\ref{tab:cifar_extended_results}.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Méthode} & \textbf{Précision test (\%)} & \textbf{Amélioration} \\
\hline
Recherche aléatoire & 82.68 & +2.73 points \\
Algorithme génétique & \textbf{85.41} & +2.04 points \\
Algorithme Firefly & 83.52 & +3.77 points \\
\hline
\end{tabular}
\caption{Performances avec entraînement étendu sur CIFAR-10}
\label{tab:cifar_extended_results}
\end{table}

\paragraph{Analyse des courbes de convergence}

Les figures~\ref{fig:cifar_convergence} illustrent le comportement des différents algorithmes au cours du processus d'optimisation.

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{front/best_fitness_compar_cifar_20250610-203202.png}
        \caption*{(a) Évolution du meilleur score}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{front/avg_fitness_compar_cifar_20250610-203202.png}
        \caption*{(b) Score moyen de la population}
    \end{minipage}
    \caption{Comparaison des courbes de convergence sur CIFAR-10}
    \label{fig:cifar_convergence}
\end{figure}

L'analyse des courbes de convergence révèle des patterns distincts entre les algorithmes. L'algorithme génétique présente une progression régulière et soutenue du meilleur score, atteignant une convergence stable vers la génération 12. La recherche aléatoire, en revanche, montre une stagnation précoce avec des améliorations marginales après les premières générations.

Le score moyen de la population (figure~\ref{fig:cifar_convergence}(b)) témoigne de la capacité des algorithmes métaheuristiques à maintenir une diversité de solutions de qualité. L'algorithme génétique et l'algorithme Firefly maintiennent des populations significativement plus performantes que la recherche aléatoire, avec des scores moyens respectifs de 0.55-0.60 contre 0.25-0.35 pour la recherche aléatoire.

\paragraph{Analyse comparative et significance statistique}

Les résultats sur CIFAR-10 confirment et amplifient les tendances observées sur MNIST. L'amélioration de 2.73 points de pourcentage en précision de test obtenue par l'algorithme génétique par rapport à la recherche aléatoire représente un gain relatif de 3.3\%, ce qui constitue une amélioration substantielle dans le contexte de la classification d'images naturelles.

La robustesse de l'algorithme génétique se manifeste également par sa capacité à découvrir des architectures plus équilibrées, comme en témoigne l'écart réduit entre les performances d'entraînement et de test (77.94\% vs 85.41\%), suggérant une meilleure capacité de généralisation.

L'efficacité computationnelle constitue un autre avantage notable : bien que l'algorithme génétique nécessite plus d'évaluations que la recherche aléatoire (278 vs 194), il demeure plus efficace que l'algorithme Firefly (327 évaluations) tout en obtenant des performances supérieures.

Ces résultats démontrent l'intérêt pratique des algorithmes génétiques pour l'optimisation d'architectures neurales sur des tâches de vision par ordinateur de complexité réelle, où les gains de performance peuvent avoir un impact significatif sur les applications pratiques.