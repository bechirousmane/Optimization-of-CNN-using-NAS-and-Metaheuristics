@article{lecun1989backpropagation,
  title={Backpropagation applied to handwritten zip code recognition},
  author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal={Neural computation},
  volume={1},
  number={4},
  pages={541--551},
  year={1989},
  publisher={MIT Press}
}

@article{zoph2016neural,
  title={Neural architecture search with reinforcement learning},
  author={Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1611.01578},
  year={2016}
}

@article{JMLR:v13:bergstra12a,
  author  = {James Bergstra and Yoshua Bengio},
  title   = {Random Search for Hyper-Parameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {10},
  pages   = {281--305},
  url     = {http://jmlr.org/papers/v13/bergstra12a.html}
}

@inproceedings{kennedy1995particle,
  title={Particle swarm optimization},
  author={Kennedy, James and Eberhart, Russell},
  booktitle={Proceedings of ICNN'95-international conference on neural networks},
  volume={4},
  pages={1942--1948},
  year={1995},
  organization={ieee}
}

@article{cheraghalipour2018tree,
  title={Tree Growth Algorithm (TGA): A novel approach for solving optimization problems},
  author={Cheraghalipour, Armin and Hajiaghaei-Keshteli, Mostafa and Paydar, Mohammad Mahdi},
  journal={Engineering Applications of Artificial Intelligence},
  volume={72},
  pages={393--414},
  year={2018},
  publisher={Elsevier}
}

@article{mockus1998application,
  title={The application of Bayesian methods for seeking the extremum},
  author={Mockus, Jonas},
  journal={Towards global optimization},
  volume={2},
  pages={117},
  year={1998}
}

@article{WU201926,
title = {Hyperparameter Optimization for Machine Learning Models Based on Bayesian Optimizationb},
journal = {Journal of Electronic Science and Technology},
volume = {17},
number = {1},
pages = {26-40},
year = {2019},
issn = {1674-862X},
doi = {https://doi.org/10.11989/JEST.1674-862X.80904120},
url = {https://www.sciencedirect.com/science/article/pii/S1674862X19300047},
author = {Jia Wu and Xiu-Yun Chen and Hao Zhang and Li-Dong Xiong and Hang Lei and Si-Hao Deng},
keywords = {Bayesian optimization, Gaussian process, hyperparameter optimization, machine learning},
abstract = {Hyperparameters are important for machine learning algorithms since they directly control the behaviors of training algorithms and have a significant effect on the performance of machine learning models. Several techniques have been developed and successfully applied for certain application domains. However, this work demands professional knowledge and expert experience. And sometimes it has to resort to the brute-force search. Therefore, if an efficient hyperparameter optimization algorithm can be developed to optimize any given machine learning method, it will greatly improve the efficiency of machine learning. In this paper, we consider building the relationship between the performance of the machine learning models and their hyperparameters by Gaussian processes. In this way, the hyperparameter tuning problem can be abstracted as an optimization problem and Bayesian optimization is used to solve the problem. Bayesian optimization is based on the Bayesian theorem. It sets a prior over the optimization function and gathers the information from the previous sample to update the posterior of the optimization function. A utility function selects the next sample point to maximize the optimization function. Several experiments were conducted on standard test datasets. Experiment results show that the proposed method can find the best hyperparameters for the widely used machine learning models, such as the random forest algorithm and the neural networks, even multi-grained cascade forest under the consideration of time cost.}
}

@InProceedings{Liu_2018_ECCV,
author = {Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li-Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},
title = {Progressive Neural Architecture Search},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
} 



@InProceedings{pmlr-v97-tan19a,
  title = 	 {{E}fficient{N}et: Rethinking Model Scaling for Convolutional Neural Networks},
  author =       {Tan, Mingxing and Le, Quoc},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6105--6114},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/tan19a/tan19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/tan19a.html},
  abstract = 	 {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flower (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.}
}


@inproceedings{np-dure,
author = {Cook, Stephen A.},
title = {The complexity of theorem-proving procedures},
year = {1971},
isbn = {9781450374644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800157.805047},
doi = {10.1145/800157.805047},
abstract = {It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be “reduced” to the problem of determining whether a given propositional formula is a tautology. Here “reduced” means, roughly speaking, that the first problem can be solved deterministically in polynomial time provided an oracle is available for solving the second. From this notion of reducible, polynomial degrees of difficulty are defined, and it is shown that the problem of determining tautologyhood has the same polynomial degree as the problem of determining whether the first of two given graphs is isomorphic to a subgraph of the second. Other examples are discussed. A method of measuring the complexity of proof procedures for the predicate calculus is introduced and discussed.},
booktitle = {Proceedings of the Third Annual ACM Symposium on Theory of Computing},
pages = {151–158},
numpages = {8},
location = {Shaker Heights, Ohio, USA},
series = {STOC '71}
}

@INPROCEEDINGS{4630852,
  author={Hingee, Kassel and Hutter, Marcus},
  booktitle={2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)}, 
  title={Equivalence of probabilistic tournament and polynomial ranking selection}, 
  year={2008},
  volume={},
  number={},
  pages={564-571},
  keywords={Polynomials;Probabilistic logic;Evolutionary computation;Indexes;Matrix decomposition;Mathematics;Gain},
  doi={10.1109/CEC.2008.4630852}}

@Inbook{MarquezCasillas2021,
author="Marquez Casillas, Edgar Saul
and Osuna-Enciso, Valent{\'i}n",
editor="Oliva, Diego
and Houssein, Essam H.
and Hinojosa, Salvador",
title="Architecture Optimization of Convolutional Neural Networks by Micro Genetic Algorithms",
bookTitle="Metaheuristics in Machine Learning: Theory and Applications",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="149--167",
abstract="Convolutional Neural Networks (CNN) are novel techniques with significant performance in object detection and classification. An open research problem on CNN is the automatic finding of adequate architectures, which is usually done by hand. Metaheuristic algorithms are techniques that find optimal solutions in heuristic manner to problems where the knowledge is limited or almost nonexistent, such as finding optimal CNN architectures. In this chapter, we propose a framework that utilizes the micro genetic algorithm to find CNN architectures in the shortest possible time. The proposal is tested over three simple study cases known by the research community (MNIST, MNIST-Fashion, and MNIST-RB), and compared against two different frameworks from the literature: psoCNN, and simple genetic algorithm. The results show a better performance of the architectures found by our framework in terms of accuracy and processing time.",
isbn="978-3-030-70542-8",
doi="10.1007/978-3-030-70542-8_7",
url="https://doi.org/10.1007/978-3-030-70542-8_7"
}


@InProceedings{FAfoundation,
author="Yang, Xin-She",
editor="Watanabe, Osamu
and Zeugmann, Thomas",
title="Firefly Algorithms for Multimodal Optimization",
booktitle="Stochastic Algorithms: Foundations and Applications",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="169--178",
abstract="Nature-inspired algorithms are among the most powerful algorithms for optimization. This paper intends to provide a detailed description of a new Firefly Algorithm (FA) for multimodal optimization applications. We will compare the proposed firefly algorithm with other metaheuristic algorithms such as particle swarm optimization (PSO). Simulations and results indicate that the proposed firefly algorithm is superior to existing metaheuristic algorithms. Finally we will discuss its applications and implications for further research.",
isbn="978-3-642-04944-6"
}

@INPROCEEDINGS{8740818,
  author={Strumberger, Ivana and Tuba, Eva and Bacanin, Nebojsa and Zivkovic, Miodrag and Beko, Marko and Tuba, Milan},
  booktitle={2019 International Young Engineers Forum (YEF-ECE)}, 
  title={Designing Convolutional Neural Network Architecture by the Firefly Algorithm}, 
  year={2019},
  volume={},
  number={},
  pages={59-65},
  keywords={Optimization;Particle swarm optimization;Task analysis;Kernel;Computer architecture;Convolutional neural networks;swarm intelligence;firefly algorithm;convolutional networks;deep learning},
  doi={10.1109/YEF-ECE.2019.8740818}}


@ARTICLE{726791,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  keywords={Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
  doi={10.1109/5.726791}}

@inproceedings{GenCNN,
  title={Genetic cnn},
  author={Xie, Lingxi and Yuille, Alan},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1379--1388},
  year={2017}
}


@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@article{10.5555/2188385.2188395,
author = {Bergstra, James and Bengio, Yoshua},
title = {Random search for hyper-parameter optimization},
year = {2012},
issue_date = {3/1/2012},
publisher = {JMLR.org},
volume = {13},
number = {null},
issn = {1532-4435},
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
journal = {J. Mach. Learn. Res.},
month = feb,
pages = {281–305},
numpages = {25},
keywords = {response surface modeling, neural networks, model selection, global optimization, deep learning}
}

@article{WEN2022101191,
title = {A new genetic algorithm based evolutionary neural architecture search for image classification},
journal = {Swarm and Evolutionary Computation},
volume = {75},
pages = {101191},
year = {2022},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2022.101191},
url = {https://www.sciencedirect.com/science/article/pii/S2210650222001547},
author = {Long Wen and Liang Gao and Xinyu Li and Hui Li},
keywords = {Neural Architecture Search, Genetic Algorithm, Deep Learning, Image Classification},
abstract = {Deep Learning (DL) has achieved the great breakthrough in image classification. As DL structure is problem-dependent and it has the crucial impact on its performance, it is still necessary to re-design the structures of DL according to the actual needs, even there exists various benchmark DL structures. Neural Architecture Search (NAS) which can design the DL network automatically has been widely investigated. However, many NAS methods suffer from the huge computation time. To overcome this drawback, this research proposed a new Evolutionary Neural Architecture Search with RepVGG nodes (EvoNAS-Rep). Firstly, a new encoding strategy is developed, which can map the fixed-length encoding individual to DL structure with variable depth using RepVGG nodes. Secondly, Genetic Algorithm (GA) is adopted for searching the optimal individual and its corresponding DL model. Thirdly, the iterative training process is designed to train the DL model and to evolve the GA simultaneously. The proposed EvoNAS-Rep is validated on the famous CIFAR 10 and CIFAR 100. The results show that EvoNAS-Rep has obtained 96.35% and 79.82% with only near 0.2 GPU days, which is both effectiveness and efficiency. EvoNAS-Rep is also validated on two real-world applications, including the NEU-CLS and the Chest XRay 2017 datasets, and the results show that EvoNAS-Rep has achieved the competitive results.}
}

@InProceedings{deepSwarm,
author="Byla, Edvinas
and Pang, Wei",
editor="Ju, Zhaojie
and Yang, Longzhi
and Yang, Chenguang
and Gegov, Alexander
and Zhou, Dalin",
title="DeepSwarm: Optimising Convolutional Neural Networks Using Swarm Intelligence",
booktitle="Advances in Computational Intelligence Systems",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="119--130",
abstract="In this paper we propose DeepSwarm, a novel neural architecture search (NAS) method based on Swarm Intelligence principles. At its core DeepSwarm uses Ant Colony Optimization (ACO) to generate ant population which uses the pheromone information to collectively search for the best neural architecture. Furthermore, by using local and global pheromone update rules our method ensures the balance between exploitation and exploration. On top of this, to make our method more efficient we combine progressive neural architecture search with weight reusability. Furthermore, due to the nature of ACO our method can incorporate heuristic information which can further speed up the search process. After systematic and extensive evaluation, we discover that on three different datasets (MNIST, Fashion-MNIST, and CIFAR-10) when compared to existing systems our proposed method demonstrates competitive performance. Finally, we open source DeepSwarm (https://github.com/Pattio/DeepSwarm) as a NAS library and hope it can be used by more deep learning researchers and practitioners.",
isbn="978-3-030-29933-0"
}

@article{bacanin2020optimizing,
  title={Optimizing convolutional neural network hyperparameters by enhanced swarm intelligence metaheuristics},
  author={Bacanin, Nebojsa and Bezdan, Timea and Tuba, Eva and Strumberger, Ivana and Tuba, Milan},
  journal={Algorithms},
  volume={13},
  number={3},
  pages={67},
  year={2020},
  publisher={MDPI}
}

@article{ACO,
  title={Ant colony optimization: Introduction and recent trends},
  author={Blum, Christian},
  journal={Physics of Life reviews},
  volume={2},
  number={4},
  pages={353--373},
  year={2005},
  publisher={Elsevier}
}

@inproceedings{pham2018efficient,
  title={Efficient neural architecture search via parameters sharing},
  author={Pham, Hieu and Guan, Melody and Zoph, Barret and Le, Quoc and Dean, Jeff},
  booktitle={International conference on machine learning},
  pages={4095--4104},
  year={2018},
  organization={PMLR}
}

@inproceedings{klein2017fast,
  title={Fast bayesian optimization of machine learning hyperparameters on large datasets},
  author={Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank},
  booktitle={Artificial intelligence and statistics},
  pages={528--536},
  year={2017},
  organization={PMLR}
}